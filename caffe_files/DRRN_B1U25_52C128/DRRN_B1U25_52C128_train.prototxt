name: "DRRN_B1U25_52C128"
layer {
    name: "data"
    type: "HDF5Data"
    top: "data"
    top: "label"
    hdf5_data_param {
        source: "../../data/train_31_x234.txt"
        batch_size: 128
        shuffle: true
    }
    include: { phase: TRAIN }
}
layer {
    name: "data"
    type: "HDF5Data"
    top: "data"
    top: "label"
    hdf5_data_param {
        source: "./../data/test_31_x234.txt"
        batch_size: 128
        shuffle: true
    }
    include: { phase: TEST }
}
layer {
    name: "bn_conv1"
    type: "BatchNorm"
    bottom: "data"
    top: "bn_conv1"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1"
    type: "BatchNorm"
    bottom: "data"
    top: "bn_conv1"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "bn_conv1"
    top: "bn_conv1"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1"
    type: "ReLU"
    bottom: "bn_conv1"
    top: "bn_conv1"
}
layer {
    name: "conv1"
    type: "Convolution"
    bottom: "bn_conv1"
    top: "conv1"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_1a"
    type: "BatchNorm"
    bottom: "conv1"
    top: "bn_conv1_1a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_1a"
    type: "BatchNorm"
    bottom: "conv1"
    top: "bn_conv1_1a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_1a"
    type: "Scale"
    bottom: "bn_conv1_1a"
    top: "bn_conv1_1a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_1a"
    type: "ReLU"
    bottom: "bn_conv1_1a"
    top: "bn_conv1_1a"
}
layer {
    name: "conv1_1a"
    type: "Convolution"
    bottom: "bn_conv1_1a"
    top: "conv1_1a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_1b"
    type: "BatchNorm"
    bottom: "conv1_1a"
    top: "bn_conv1_1b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_1b"
    type: "BatchNorm"
    bottom: "conv1_1a"
    top: "bn_conv1_1b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_1b"
    type: "Scale"
    bottom: "bn_conv1_1b"
    top: "bn_conv1_1b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_1b"
    type: "ReLU"
    bottom: "bn_conv1_1b"
    top: "bn_conv1_1b"
}
layer {
    name: "conv1_1b"
    type: "Convolution"
    bottom: "bn_conv1_1b"
    top: "conv1_1b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_1"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_1b"
    top: "eltwise1_1"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_2a"
    type: "BatchNorm"
    bottom: "eltwise1_1"
    top: "bn_conv1_2a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_2a"
    type: "BatchNorm"
    bottom: "eltwise1_1"
    top: "bn_conv1_2a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_2a"
    type: "Scale"
    bottom: "bn_conv1_2a"
    top: "bn_conv1_2a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_2a"
    type: "ReLU"
    bottom: "bn_conv1_2a"
    top: "bn_conv1_2a"
}
layer {
    name: "conv1_2a"
    type: "Convolution"
    bottom: "bn_conv1_2a"
    top: "conv1_2a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_2b"
    type: "BatchNorm"
    bottom: "conv1_2a"
    top: "bn_conv1_2b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_2b"
    type: "BatchNorm"
    bottom: "conv1_2a"
    top: "bn_conv1_2b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_2b"
    type: "Scale"
    bottom: "bn_conv1_2b"
    top: "bn_conv1_2b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_2b"
    type: "ReLU"
    bottom: "bn_conv1_2b"
    top: "bn_conv1_2b"
}
layer {
    name: "conv1_2b"
    type: "Convolution"
    bottom: "bn_conv1_2b"
    top: "conv1_2b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_2"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_2b"
    top: "eltwise1_2"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_3a"
    type: "BatchNorm"
    bottom: "eltwise1_2"
    top: "bn_conv1_3a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_3a"
    type: "BatchNorm"
    bottom: "eltwise1_2"
    top: "bn_conv1_3a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_3a"
    type: "Scale"
    bottom: "bn_conv1_3a"
    top: "bn_conv1_3a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_3a"
    type: "ReLU"
    bottom: "bn_conv1_3a"
    top: "bn_conv1_3a"
}
layer {
    name: "conv1_3a"
    type: "Convolution"
    bottom: "bn_conv1_3a"
    top: "conv1_3a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_3b"
    type: "BatchNorm"
    bottom: "conv1_3a"
    top: "bn_conv1_3b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_3b"
    type: "BatchNorm"
    bottom: "conv1_3a"
    top: "bn_conv1_3b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_3b"
    type: "Scale"
    bottom: "bn_conv1_3b"
    top: "bn_conv1_3b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_3b"
    type: "ReLU"
    bottom: "bn_conv1_3b"
    top: "bn_conv1_3b"
}
layer {
    name: "conv1_3b"
    type: "Convolution"
    bottom: "bn_conv1_3b"
    top: "conv1_3b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_3"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_3b"
    top: "eltwise1_3"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_4a"
    type: "BatchNorm"
    bottom: "eltwise1_3"
    top: "bn_conv1_4a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_4a"
    type: "BatchNorm"
    bottom: "eltwise1_3"
    top: "bn_conv1_4a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_4a"
    type: "Scale"
    bottom: "bn_conv1_4a"
    top: "bn_conv1_4a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_4a"
    type: "ReLU"
    bottom: "bn_conv1_4a"
    top: "bn_conv1_4a"
}
layer {
    name: "conv1_4a"
    type: "Convolution"
    bottom: "bn_conv1_4a"
    top: "conv1_4a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_4b"
    type: "BatchNorm"
    bottom: "conv1_4a"
    top: "bn_conv1_4b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_4b"
    type: "BatchNorm"
    bottom: "conv1_4a"
    top: "bn_conv1_4b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_4b"
    type: "Scale"
    bottom: "bn_conv1_4b"
    top: "bn_conv1_4b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_4b"
    type: "ReLU"
    bottom: "bn_conv1_4b"
    top: "bn_conv1_4b"
}
layer {
    name: "conv1_4b"
    type: "Convolution"
    bottom: "bn_conv1_4b"
    top: "conv1_4b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_4"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_4b"
    top: "eltwise1_4"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_5a"
    type: "BatchNorm"
    bottom: "eltwise1_4"
    top: "bn_conv1_5a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_5a"
    type: "BatchNorm"
    bottom: "eltwise1_4"
    top: "bn_conv1_5a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_5a"
    type: "Scale"
    bottom: "bn_conv1_5a"
    top: "bn_conv1_5a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_5a"
    type: "ReLU"
    bottom: "bn_conv1_5a"
    top: "bn_conv1_5a"
}
layer {
    name: "conv1_5a"
    type: "Convolution"
    bottom: "bn_conv1_5a"
    top: "conv1_5a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_5b"
    type: "BatchNorm"
    bottom: "conv1_5a"
    top: "bn_conv1_5b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_5b"
    type: "BatchNorm"
    bottom: "conv1_5a"
    top: "bn_conv1_5b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_5b"
    type: "Scale"
    bottom: "bn_conv1_5b"
    top: "bn_conv1_5b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_5b"
    type: "ReLU"
    bottom: "bn_conv1_5b"
    top: "bn_conv1_5b"
}
layer {
    name: "conv1_5b"
    type: "Convolution"
    bottom: "bn_conv1_5b"
    top: "conv1_5b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_5"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_5b"
    top: "eltwise1_5"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_6a"
    type: "BatchNorm"
    bottom: "eltwise1_5"
    top: "bn_conv1_6a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_6a"
    type: "BatchNorm"
    bottom: "eltwise1_5"
    top: "bn_conv1_6a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_6a"
    type: "Scale"
    bottom: "bn_conv1_6a"
    top: "bn_conv1_6a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_6a"
    type: "ReLU"
    bottom: "bn_conv1_6a"
    top: "bn_conv1_6a"
}
layer {
    name: "conv1_6a"
    type: "Convolution"
    bottom: "bn_conv1_6a"
    top: "conv1_6a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_6b"
    type: "BatchNorm"
    bottom: "conv1_6a"
    top: "bn_conv1_6b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_6b"
    type: "BatchNorm"
    bottom: "conv1_6a"
    top: "bn_conv1_6b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_6b"
    type: "Scale"
    bottom: "bn_conv1_6b"
    top: "bn_conv1_6b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_6b"
    type: "ReLU"
    bottom: "bn_conv1_6b"
    top: "bn_conv1_6b"
}
layer {
    name: "conv1_6b"
    type: "Convolution"
    bottom: "bn_conv1_6b"
    top: "conv1_6b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_6"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_6b"
    top: "eltwise1_6"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_7a"
    type: "BatchNorm"
    bottom: "eltwise1_6"
    top: "bn_conv1_7a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_7a"
    type: "BatchNorm"
    bottom: "eltwise1_6"
    top: "bn_conv1_7a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_7a"
    type: "Scale"
    bottom: "bn_conv1_7a"
    top: "bn_conv1_7a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_7a"
    type: "ReLU"
    bottom: "bn_conv1_7a"
    top: "bn_conv1_7a"
}
layer {
    name: "conv1_7a"
    type: "Convolution"
    bottom: "bn_conv1_7a"
    top: "conv1_7a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_7b"
    type: "BatchNorm"
    bottom: "conv1_7a"
    top: "bn_conv1_7b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_7b"
    type: "BatchNorm"
    bottom: "conv1_7a"
    top: "bn_conv1_7b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_7b"
    type: "Scale"
    bottom: "bn_conv1_7b"
    top: "bn_conv1_7b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_7b"
    type: "ReLU"
    bottom: "bn_conv1_7b"
    top: "bn_conv1_7b"
}
layer {
    name: "conv1_7b"
    type: "Convolution"
    bottom: "bn_conv1_7b"
    top: "conv1_7b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_7"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_7b"
    top: "eltwise1_7"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_8a"
    type: "BatchNorm"
    bottom: "eltwise1_7"
    top: "bn_conv1_8a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_8a"
    type: "BatchNorm"
    bottom: "eltwise1_7"
    top: "bn_conv1_8a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_8a"
    type: "Scale"
    bottom: "bn_conv1_8a"
    top: "bn_conv1_8a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_8a"
    type: "ReLU"
    bottom: "bn_conv1_8a"
    top: "bn_conv1_8a"
}
layer {
    name: "conv1_8a"
    type: "Convolution"
    bottom: "bn_conv1_8a"
    top: "conv1_8a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_8b"
    type: "BatchNorm"
    bottom: "conv1_8a"
    top: "bn_conv1_8b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_8b"
    type: "BatchNorm"
    bottom: "conv1_8a"
    top: "bn_conv1_8b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_8b"
    type: "Scale"
    bottom: "bn_conv1_8b"
    top: "bn_conv1_8b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_8b"
    type: "ReLU"
    bottom: "bn_conv1_8b"
    top: "bn_conv1_8b"
}
layer {
    name: "conv1_8b"
    type: "Convolution"
    bottom: "bn_conv1_8b"
    top: "conv1_8b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_8"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_8b"
    top: "eltwise1_8"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_9a"
    type: "BatchNorm"
    bottom: "eltwise1_8"
    top: "bn_conv1_9a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_9a"
    type: "BatchNorm"
    bottom: "eltwise1_8"
    top: "bn_conv1_9a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_9a"
    type: "Scale"
    bottom: "bn_conv1_9a"
    top: "bn_conv1_9a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_9a"
    type: "ReLU"
    bottom: "bn_conv1_9a"
    top: "bn_conv1_9a"
}
layer {
    name: "conv1_9a"
    type: "Convolution"
    bottom: "bn_conv1_9a"
    top: "conv1_9a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_9b"
    type: "BatchNorm"
    bottom: "conv1_9a"
    top: "bn_conv1_9b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_9b"
    type: "BatchNorm"
    bottom: "conv1_9a"
    top: "bn_conv1_9b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_9b"
    type: "Scale"
    bottom: "bn_conv1_9b"
    top: "bn_conv1_9b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_9b"
    type: "ReLU"
    bottom: "bn_conv1_9b"
    top: "bn_conv1_9b"
}
layer {
    name: "conv1_9b"
    type: "Convolution"
    bottom: "bn_conv1_9b"
    top: "conv1_9b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_9"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_9b"
    top: "eltwise1_9"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_10a"
    type: "BatchNorm"
    bottom: "eltwise1_9"
    top: "bn_conv1_10a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_10a"
    type: "BatchNorm"
    bottom: "eltwise1_9"
    top: "bn_conv1_10a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_10a"
    type: "Scale"
    bottom: "bn_conv1_10a"
    top: "bn_conv1_10a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_10a"
    type: "ReLU"
    bottom: "bn_conv1_10a"
    top: "bn_conv1_10a"
}
layer {
    name: "conv1_10a"
    type: "Convolution"
    bottom: "bn_conv1_10a"
    top: "conv1_10a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_10b"
    type: "BatchNorm"
    bottom: "conv1_10a"
    top: "bn_conv1_10b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_10b"
    type: "BatchNorm"
    bottom: "conv1_10a"
    top: "bn_conv1_10b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_10b"
    type: "Scale"
    bottom: "bn_conv1_10b"
    top: "bn_conv1_10b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_10b"
    type: "ReLU"
    bottom: "bn_conv1_10b"
    top: "bn_conv1_10b"
}
layer {
    name: "conv1_10b"
    type: "Convolution"
    bottom: "bn_conv1_10b"
    top: "conv1_10b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_10"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_10b"
    top: "eltwise1_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_11a"
    type: "BatchNorm"
    bottom: "eltwise1_10"
    top: "bn_conv1_11a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_11a"
    type: "BatchNorm"
    bottom: "eltwise1_10"
    top: "bn_conv1_11a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_11a"
    type: "Scale"
    bottom: "bn_conv1_11a"
    top: "bn_conv1_11a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_11a"
    type: "ReLU"
    bottom: "bn_conv1_11a"
    top: "bn_conv1_11a"
}
layer {
    name: "conv1_11a"
    type: "Convolution"
    bottom: "bn_conv1_11a"
    top: "conv1_11a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_11b"
    type: "BatchNorm"
    bottom: "conv1_11a"
    top: "bn_conv1_11b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_11b"
    type: "BatchNorm"
    bottom: "conv1_11a"
    top: "bn_conv1_11b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_11b"
    type: "Scale"
    bottom: "bn_conv1_11b"
    top: "bn_conv1_11b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_11b"
    type: "ReLU"
    bottom: "bn_conv1_11b"
    top: "bn_conv1_11b"
}
layer {
    name: "conv1_11b"
    type: "Convolution"
    bottom: "bn_conv1_11b"
    top: "conv1_11b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_11"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_11b"
    top: "eltwise1_11"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_12a"
    type: "BatchNorm"
    bottom: "eltwise1_11"
    top: "bn_conv1_12a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_12a"
    type: "BatchNorm"
    bottom: "eltwise1_11"
    top: "bn_conv1_12a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_12a"
    type: "Scale"
    bottom: "bn_conv1_12a"
    top: "bn_conv1_12a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_12a"
    type: "ReLU"
    bottom: "bn_conv1_12a"
    top: "bn_conv1_12a"
}
layer {
    name: "conv1_12a"
    type: "Convolution"
    bottom: "bn_conv1_12a"
    top: "conv1_12a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_12b"
    type: "BatchNorm"
    bottom: "conv1_12a"
    top: "bn_conv1_12b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_12b"
    type: "BatchNorm"
    bottom: "conv1_12a"
    top: "bn_conv1_12b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_12b"
    type: "Scale"
    bottom: "bn_conv1_12b"
    top: "bn_conv1_12b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_12b"
    type: "ReLU"
    bottom: "bn_conv1_12b"
    top: "bn_conv1_12b"
}
layer {
    name: "conv1_12b"
    type: "Convolution"
    bottom: "bn_conv1_12b"
    top: "conv1_12b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_12"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_12b"
    top: "eltwise1_12"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_13a"
    type: "BatchNorm"
    bottom: "eltwise1_12"
    top: "bn_conv1_13a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_13a"
    type: "BatchNorm"
    bottom: "eltwise1_12"
    top: "bn_conv1_13a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_13a"
    type: "Scale"
    bottom: "bn_conv1_13a"
    top: "bn_conv1_13a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_13a"
    type: "ReLU"
    bottom: "bn_conv1_13a"
    top: "bn_conv1_13a"
}
layer {
    name: "conv1_13a"
    type: "Convolution"
    bottom: "bn_conv1_13a"
    top: "conv1_13a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_13b"
    type: "BatchNorm"
    bottom: "conv1_13a"
    top: "bn_conv1_13b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_13b"
    type: "BatchNorm"
    bottom: "conv1_13a"
    top: "bn_conv1_13b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_13b"
    type: "Scale"
    bottom: "bn_conv1_13b"
    top: "bn_conv1_13b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_13b"
    type: "ReLU"
    bottom: "bn_conv1_13b"
    top: "bn_conv1_13b"
}
layer {
    name: "conv1_13b"
    type: "Convolution"
    bottom: "bn_conv1_13b"
    top: "conv1_13b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_13"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_13b"
    top: "eltwise1_13"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_14a"
    type: "BatchNorm"
    bottom: "eltwise1_13"
    top: "bn_conv1_14a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_14a"
    type: "BatchNorm"
    bottom: "eltwise1_13"
    top: "bn_conv1_14a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_14a"
    type: "Scale"
    bottom: "bn_conv1_14a"
    top: "bn_conv1_14a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_14a"
    type: "ReLU"
    bottom: "bn_conv1_14a"
    top: "bn_conv1_14a"
}
layer {
    name: "conv1_14a"
    type: "Convolution"
    bottom: "bn_conv1_14a"
    top: "conv1_14a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_14b"
    type: "BatchNorm"
    bottom: "conv1_14a"
    top: "bn_conv1_14b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_14b"
    type: "BatchNorm"
    bottom: "conv1_14a"
    top: "bn_conv1_14b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_14b"
    type: "Scale"
    bottom: "bn_conv1_14b"
    top: "bn_conv1_14b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_14b"
    type: "ReLU"
    bottom: "bn_conv1_14b"
    top: "bn_conv1_14b"
}
layer {
    name: "conv1_14b"
    type: "Convolution"
    bottom: "bn_conv1_14b"
    top: "conv1_14b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_14"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_14b"
    top: "eltwise1_14"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_15a"
    type: "BatchNorm"
    bottom: "eltwise1_14"
    top: "bn_conv1_15a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_15a"
    type: "BatchNorm"
    bottom: "eltwise1_14"
    top: "bn_conv1_15a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_15a"
    type: "Scale"
    bottom: "bn_conv1_15a"
    top: "bn_conv1_15a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_15a"
    type: "ReLU"
    bottom: "bn_conv1_15a"
    top: "bn_conv1_15a"
}
layer {
    name: "conv1_15a"
    type: "Convolution"
    bottom: "bn_conv1_15a"
    top: "conv1_15a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_15b"
    type: "BatchNorm"
    bottom: "conv1_15a"
    top: "bn_conv1_15b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_15b"
    type: "BatchNorm"
    bottom: "conv1_15a"
    top: "bn_conv1_15b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_15b"
    type: "Scale"
    bottom: "bn_conv1_15b"
    top: "bn_conv1_15b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_15b"
    type: "ReLU"
    bottom: "bn_conv1_15b"
    top: "bn_conv1_15b"
}
layer {
    name: "conv1_15b"
    type: "Convolution"
    bottom: "bn_conv1_15b"
    top: "conv1_15b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_15"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_15b"
    top: "eltwise1_15"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_16a"
    type: "BatchNorm"
    bottom: "eltwise1_15"
    top: "bn_conv1_16a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_16a"
    type: "BatchNorm"
    bottom: "eltwise1_15"
    top: "bn_conv1_16a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_16a"
    type: "Scale"
    bottom: "bn_conv1_16a"
    top: "bn_conv1_16a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_16a"
    type: "ReLU"
    bottom: "bn_conv1_16a"
    top: "bn_conv1_16a"
}
layer {
    name: "conv1_16a"
    type: "Convolution"
    bottom: "bn_conv1_16a"
    top: "conv1_16a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_16b"
    type: "BatchNorm"
    bottom: "conv1_16a"
    top: "bn_conv1_16b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_16b"
    type: "BatchNorm"
    bottom: "conv1_16a"
    top: "bn_conv1_16b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_16b"
    type: "Scale"
    bottom: "bn_conv1_16b"
    top: "bn_conv1_16b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_16b"
    type: "ReLU"
    bottom: "bn_conv1_16b"
    top: "bn_conv1_16b"
}
layer {
    name: "conv1_16b"
    type: "Convolution"
    bottom: "bn_conv1_16b"
    top: "conv1_16b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_16"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_16b"
    top: "eltwise1_16"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_17a"
    type: "BatchNorm"
    bottom: "eltwise1_16"
    top: "bn_conv1_17a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_17a"
    type: "BatchNorm"
    bottom: "eltwise1_16"
    top: "bn_conv1_17a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_17a"
    type: "Scale"
    bottom: "bn_conv1_17a"
    top: "bn_conv1_17a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_17a"
    type: "ReLU"
    bottom: "bn_conv1_17a"
    top: "bn_conv1_17a"
}
layer {
    name: "conv1_17a"
    type: "Convolution"
    bottom: "bn_conv1_17a"
    top: "conv1_17a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_17b"
    type: "BatchNorm"
    bottom: "conv1_17a"
    top: "bn_conv1_17b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_17b"
    type: "BatchNorm"
    bottom: "conv1_17a"
    top: "bn_conv1_17b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_17b"
    type: "Scale"
    bottom: "bn_conv1_17b"
    top: "bn_conv1_17b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_17b"
    type: "ReLU"
    bottom: "bn_conv1_17b"
    top: "bn_conv1_17b"
}
layer {
    name: "conv1_17b"
    type: "Convolution"
    bottom: "bn_conv1_17b"
    top: "conv1_17b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_17"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_17b"
    top: "eltwise1_17"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_18a"
    type: "BatchNorm"
    bottom: "eltwise1_17"
    top: "bn_conv1_18a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_18a"
    type: "BatchNorm"
    bottom: "eltwise1_17"
    top: "bn_conv1_18a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_18a"
    type: "Scale"
    bottom: "bn_conv1_18a"
    top: "bn_conv1_18a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_18a"
    type: "ReLU"
    bottom: "bn_conv1_18a"
    top: "bn_conv1_18a"
}
layer {
    name: "conv1_18a"
    type: "Convolution"
    bottom: "bn_conv1_18a"
    top: "conv1_18a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_18b"
    type: "BatchNorm"
    bottom: "conv1_18a"
    top: "bn_conv1_18b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_18b"
    type: "BatchNorm"
    bottom: "conv1_18a"
    top: "bn_conv1_18b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_18b"
    type: "Scale"
    bottom: "bn_conv1_18b"
    top: "bn_conv1_18b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_18b"
    type: "ReLU"
    bottom: "bn_conv1_18b"
    top: "bn_conv1_18b"
}
layer {
    name: "conv1_18b"
    type: "Convolution"
    bottom: "bn_conv1_18b"
    top: "conv1_18b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_18"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_18b"
    top: "eltwise1_18"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_19a"
    type: "BatchNorm"
    bottom: "eltwise1_18"
    top: "bn_conv1_19a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_19a"
    type: "BatchNorm"
    bottom: "eltwise1_18"
    top: "bn_conv1_19a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_19a"
    type: "Scale"
    bottom: "bn_conv1_19a"
    top: "bn_conv1_19a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_19a"
    type: "ReLU"
    bottom: "bn_conv1_19a"
    top: "bn_conv1_19a"
}
layer {
    name: "conv1_19a"
    type: "Convolution"
    bottom: "bn_conv1_19a"
    top: "conv1_19a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_19b"
    type: "BatchNorm"
    bottom: "conv1_19a"
    top: "bn_conv1_19b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_19b"
    type: "BatchNorm"
    bottom: "conv1_19a"
    top: "bn_conv1_19b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_19b"
    type: "Scale"
    bottom: "bn_conv1_19b"
    top: "bn_conv1_19b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_19b"
    type: "ReLU"
    bottom: "bn_conv1_19b"
    top: "bn_conv1_19b"
}
layer {
    name: "conv1_19b"
    type: "Convolution"
    bottom: "bn_conv1_19b"
    top: "conv1_19b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_19"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_19b"
    top: "eltwise1_19"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_20a"
    type: "BatchNorm"
    bottom: "eltwise1_19"
    top: "bn_conv1_20a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_20a"
    type: "BatchNorm"
    bottom: "eltwise1_19"
    top: "bn_conv1_20a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_20a"
    type: "Scale"
    bottom: "bn_conv1_20a"
    top: "bn_conv1_20a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_20a"
    type: "ReLU"
    bottom: "bn_conv1_20a"
    top: "bn_conv1_20a"
}
layer {
    name: "conv1_20a"
    type: "Convolution"
    bottom: "bn_conv1_20a"
    top: "conv1_20a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_20b"
    type: "BatchNorm"
    bottom: "conv1_20a"
    top: "bn_conv1_20b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_20b"
    type: "BatchNorm"
    bottom: "conv1_20a"
    top: "bn_conv1_20b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_20b"
    type: "Scale"
    bottom: "bn_conv1_20b"
    top: "bn_conv1_20b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_20b"
    type: "ReLU"
    bottom: "bn_conv1_20b"
    top: "bn_conv1_20b"
}
layer {
    name: "conv1_20b"
    type: "Convolution"
    bottom: "bn_conv1_20b"
    top: "conv1_20b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_20"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_20b"
    top: "eltwise1_20"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_21a"
    type: "BatchNorm"
    bottom: "eltwise1_20"
    top: "bn_conv1_21a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_21a"
    type: "BatchNorm"
    bottom: "eltwise1_20"
    top: "bn_conv1_21a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_21a"
    type: "Scale"
    bottom: "bn_conv1_21a"
    top: "bn_conv1_21a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_21a"
    type: "ReLU"
    bottom: "bn_conv1_21a"
    top: "bn_conv1_21a"
}
layer {
    name: "conv1_21a"
    type: "Convolution"
    bottom: "bn_conv1_21a"
    top: "conv1_21a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_21b"
    type: "BatchNorm"
    bottom: "conv1_21a"
    top: "bn_conv1_21b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_21b"
    type: "BatchNorm"
    bottom: "conv1_21a"
    top: "bn_conv1_21b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_21b"
    type: "Scale"
    bottom: "bn_conv1_21b"
    top: "bn_conv1_21b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_21b"
    type: "ReLU"
    bottom: "bn_conv1_21b"
    top: "bn_conv1_21b"
}
layer {
    name: "conv1_21b"
    type: "Convolution"
    bottom: "bn_conv1_21b"
    top: "conv1_21b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_21"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_21b"
    top: "eltwise1_21"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_22a"
    type: "BatchNorm"
    bottom: "eltwise1_21"
    top: "bn_conv1_22a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_22a"
    type: "BatchNorm"
    bottom: "eltwise1_21"
    top: "bn_conv1_22a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_22a"
    type: "Scale"
    bottom: "bn_conv1_22a"
    top: "bn_conv1_22a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_22a"
    type: "ReLU"
    bottom: "bn_conv1_22a"
    top: "bn_conv1_22a"
}
layer {
    name: "conv1_22a"
    type: "Convolution"
    bottom: "bn_conv1_22a"
    top: "conv1_22a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_22b"
    type: "BatchNorm"
    bottom: "conv1_22a"
    top: "bn_conv1_22b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_22b"
    type: "BatchNorm"
    bottom: "conv1_22a"
    top: "bn_conv1_22b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_22b"
    type: "Scale"
    bottom: "bn_conv1_22b"
    top: "bn_conv1_22b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_22b"
    type: "ReLU"
    bottom: "bn_conv1_22b"
    top: "bn_conv1_22b"
}
layer {
    name: "conv1_22b"
    type: "Convolution"
    bottom: "bn_conv1_22b"
    top: "conv1_22b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_22"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_22b"
    top: "eltwise1_22"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_23a"
    type: "BatchNorm"
    bottom: "eltwise1_22"
    top: "bn_conv1_23a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_23a"
    type: "BatchNorm"
    bottom: "eltwise1_22"
    top: "bn_conv1_23a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_23a"
    type: "Scale"
    bottom: "bn_conv1_23a"
    top: "bn_conv1_23a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_23a"
    type: "ReLU"
    bottom: "bn_conv1_23a"
    top: "bn_conv1_23a"
}
layer {
    name: "conv1_23a"
    type: "Convolution"
    bottom: "bn_conv1_23a"
    top: "conv1_23a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_23b"
    type: "BatchNorm"
    bottom: "conv1_23a"
    top: "bn_conv1_23b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_23b"
    type: "BatchNorm"
    bottom: "conv1_23a"
    top: "bn_conv1_23b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_23b"
    type: "Scale"
    bottom: "bn_conv1_23b"
    top: "bn_conv1_23b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_23b"
    type: "ReLU"
    bottom: "bn_conv1_23b"
    top: "bn_conv1_23b"
}
layer {
    name: "conv1_23b"
    type: "Convolution"
    bottom: "bn_conv1_23b"
    top: "conv1_23b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_23"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_23b"
    top: "eltwise1_23"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_24a"
    type: "BatchNorm"
    bottom: "eltwise1_23"
    top: "bn_conv1_24a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_24a"
    type: "BatchNorm"
    bottom: "eltwise1_23"
    top: "bn_conv1_24a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_24a"
    type: "Scale"
    bottom: "bn_conv1_24a"
    top: "bn_conv1_24a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_24a"
    type: "ReLU"
    bottom: "bn_conv1_24a"
    top: "bn_conv1_24a"
}
layer {
    name: "conv1_24a"
    type: "Convolution"
    bottom: "bn_conv1_24a"
    top: "conv1_24a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_24b"
    type: "BatchNorm"
    bottom: "conv1_24a"
    top: "bn_conv1_24b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_24b"
    type: "BatchNorm"
    bottom: "conv1_24a"
    top: "bn_conv1_24b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_24b"
    type: "Scale"
    bottom: "bn_conv1_24b"
    top: "bn_conv1_24b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_24b"
    type: "ReLU"
    bottom: "bn_conv1_24b"
    top: "bn_conv1_24b"
}
layer {
    name: "conv1_24b"
    type: "Convolution"
    bottom: "bn_conv1_24b"
    top: "conv1_24b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_24"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_24b"
    top: "eltwise1_24"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv1_25a"
    type: "BatchNorm"
    bottom: "eltwise1_24"
    top: "bn_conv1_25a"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_25a"
    type: "BatchNorm"
    bottom: "eltwise1_24"
    top: "bn_conv1_25a"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_25a"
    type: "Scale"
    bottom: "bn_conv1_25a"
    top: "bn_conv1_25a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_25a"
    type: "ReLU"
    bottom: "bn_conv1_25a"
    top: "bn_conv1_25a"
}
layer {
    name: "conv1_25a"
    type: "Convolution"
    bottom: "bn_conv1_25a"
    top: "conv1_25a"
    param {
        name: "RB1_wa"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv1_25b"
    type: "BatchNorm"
    bottom: "conv1_25a"
    top: "bn_conv1_25b"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv1_25b"
    type: "BatchNorm"
    bottom: "conv1_25a"
    top: "bn_conv1_25b"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv1_25b"
    type: "Scale"
    bottom: "bn_conv1_25b"
    top: "bn_conv1_25b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1_25b"
    type: "ReLU"
    bottom: "bn_conv1_25b"
    top: "bn_conv1_25b"
}
layer {
    name: "conv1_25b"
    type: "Convolution"
    bottom: "bn_conv1_25b"
    top: "conv1_25b"
    param {
        name: "RB1_wb"
        lr_mult: 1.000000
    }
    param {
        name: "RB1_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise1_25"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv1_25b"
    top: "eltwise1_25"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv_end"
    type: "BatchNorm"
    bottom: "eltwise1_25"
    top: "bn_conv_end"
    batch_norm_param {
        use_global_stats: false
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TRAIN
    }
}
layer {
    name: "bn_conv_end"
    type: "BatchNorm"
    bottom: "eltwise1_25"
    top: "bn_conv_end"
    batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    include {
        phase: TEST
    }
}
layer {
    name: "scale_conv_end"
    type: "Scale"
    bottom: "bn_conv_end"
    top: "bn_conv_end"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end"
    type: "ReLU"
    bottom: "bn_conv_end"
    top: "bn_conv_end"
}
layer {
    name: "conv_end"
    type: "Convolution"
    bottom: "bn_conv_end"
    top: "conv_end"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end"
    top: "HR_recovery"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "loss"
    type: "EuclideanLoss"
    bottom: "HR_recovery"
    bottom: "label"
    top: "loss"
}
